# Qwen3-TTS Deep Streaming â€” å·¥ç¨‹ SOP

> æœ¬æ–‡æ¡£é¢å‘æ–°åŠ å…¥çš„å·¥ç¨‹å¸ˆï¼Œç›®æ ‡ï¼šè®©ä½ èƒ½åœ¨ 30 åˆ†é’Ÿå†…å¯åŠ¨æœåŠ¡ã€è·‘å›å½’ã€ç†è§£ CUDA Graph åŠ é€ŸåŸç†ï¼Œå¹¶é¿å…å·²çŸ¥è¸©å‘ã€‚

---

## ç›®å½•

1. [é¡¹ç›®æ¶æ„æ¦‚è§ˆ](#1-é¡¹ç›®æ¶æ„æ¦‚è§ˆ)
2. [ç¯å¢ƒ & ä¾èµ–](#2-ç¯å¢ƒ--ä¾èµ–)
3. [å¯åŠ¨æœåŠ¡](#3-å¯åŠ¨æœåŠ¡)
4. [ç¯å¢ƒå˜é‡é€ŸæŸ¥è¡¨](#4-ç¯å¢ƒå˜é‡é€ŸæŸ¥è¡¨)
5. [å›å½’æµ‹è¯•](#5-å›å½’æµ‹è¯•)
6. [CUDA Graph åŠ é€Ÿ](#6-cuda-graph-åŠ é€Ÿ)
7. [å…³é”®æŒ‡æ ‡å®šä¹‰](#7-å…³é”®æŒ‡æ ‡å®šä¹‰)
8. [å¸¸è§é”™è¯¯ & ä¿®å¤](#8-å¸¸è§é”™è¯¯--ä¿®å¤)
9. [è¸©å‘ç»éªŒ & é“å¾‹](#9-è¸©å‘ç»éªŒ--é“å¾‹)
10. [æ–‡ä»¶ç´¢å¼•](#10-æ–‡ä»¶ç´¢å¼•)

---

## 1. é¡¹ç›®æ¶æ„æ¦‚è§ˆ

```
è¯·æ±‚ â†’ /tts/stream (FastAPI)
          â”‚
          â”œâ”€ Codegenï¼ˆTalker + Code Predictorï¼‰
          â”‚    Talker: è‡ªå›å½’ç”Ÿæˆ codec token çš„ç¬¬ 0 ç»„
          â”‚    Code Predictor (CP): 14 ä¸ª lm_head å¹¶è¡Œé¢„æµ‹å‰©ä½™ç»„
          â”‚
          â””â”€ Decoderï¼ˆIncrementalDecoderï¼‰
               quantizer â†’ pre_conv â†’ pre_transformer
               â†’ 2x (TransConv + ConvNeXt) upsample
               â†’ decoder blocks (CausalConvNet, ResidualUnit, SnakeBeta)
               â†’ PCM éŸ³é¢‘è¾“å‡º
```

### å…³é”®æ¦‚å¿µ

| æ¦‚å¿µ | è¯´æ˜ |
|---|---|
| **Deep Streaming** | å¢é‡ codegen + å¢é‡ decode â†’ å®æ—¶ PCM è¾“å‡º |
| **packet_tokens** | æ¯æ¬¡ codegen ç”Ÿæˆçš„ token æ•°ï¼ˆé»˜è®¤ 2ï¼‰ |
| **left_context** | è§£ç å™¨ä¿ç•™çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆé»˜è®¤ 72ï¼‰ |
| **Incremental Decode** | æµå¼å·ç§¯ï¼Œé€æ­¥ç”Ÿæˆ PCMï¼Œä¸éœ€è¦å®Œæ•´ codes |
| **CUDA Graph** | å°† CUDA kernel åºåˆ—"å½•åƒ+é‡æ”¾"ï¼Œå‡å°‘ CPU launch å¼€é”€ |

### æ•´ä½“ç³»ç»Ÿæ¶æ„

```
ç”¨æˆ· â†’ LLM (vLLM OpenAI API)
         â”‚ streaming text
         â”œâ”€ Bridge (æ–‡æœ¬åˆ‡åˆ†)
         â”‚    flush ç­–ç•¥ï¼šä¸­æ–‡æ ‡ç‚¹ç«‹å³ flushï¼›æ— æ ‡ç‚¹æ—¶ 8~12 å­—
         â”‚    starter æ®µï¼š2~6 å­—ä¼˜å…ˆé€ TTS é™ä½é¦–åŒ…å»¶è¿Ÿ
         â”‚
         â””â”€ TTS Server (/tts/stream)
              Deep Streaming: å¢é‡ codegen â†’ å¢é‡ decode â†’ PCM
```

---

## 2. ç¯å¢ƒ & ä¾èµ–

```bash
# GPU & Driver
GPU:   NVIDIA L40S (48 GiB)
CUDA:  12.x
Driver: 550.127.05
vLLM:  0.14.0

# Python è·¯å¾„
export PYTHONPATH="/workspace/vllm-omni"

# æ¨¡å‹è·¯å¾„
/workspace/models/Qwen3-TTS-12Hz-0.6B-CustomVoice    # â† TTS å½“å‰ä¸»åŠ›æ¨¡å‹
/workspace/models/Qwen3-TTS-12Hz-1.7B-CustomVoice    # TTS 1.7B å¤‡é€‰
/workspace/models/Qwen3-Omni-AWQ-4bit                # LLMï¼ˆAWQ 4-bit é‡åŒ–ï¼‰

# vLLM-Omniï¼ˆä»…æä¾› TTS æ¨¡å‹åŠ è½½èƒ½åŠ›ï¼Œä¸èµ° vLLM æ¨ç†å¼•æ“ï¼‰
# âš ï¸ æœ‰æœ¬åœ°è¡¥ä¸ï¼ˆCUDA Graph æ”¯æŒã€decoder forward ç­¾åç­‰ï¼‰ï¼Œä¸è¦ git checkout/reset
/workspace/vllm-omni
```

> LLMï¼ˆgpu_memory_utilization=0.6ï¼‰å ç”¨ â‰¤ 27 GiBï¼ŒTTS 0.6B å®æµ‹å ç”¨ ~3.4 GiBã€‚
> åˆè®¡ ~30.4 GiBï¼ŒL40S (45 GiB) å¯å•å¡å…±å­˜ï¼Œä½™é‡ ~14.6 GiBã€‚
> æ³¨æ„ï¼šå¦‚æœè°ƒé«˜ LLM çš„ `gpu_memory_utilization`ï¼ˆå¦‚ 0.9ï¼‰ï¼Œåˆ™å¯èƒ½æŒ¤å‹ TTS å¯¼è‡´ OOMã€‚

---

## 3. å¯åŠ¨æœåŠ¡

### 3.1 LLM Serverï¼ˆvLLM OpenAI APIï¼‰

```bash
bash "/workspace/project 1/25/scripts/run_llm_server.sh"
```

å…³é”®å‚æ•°ï¼ˆè„šæœ¬å†…å·²é…ç½®ï¼‰ï¼š
- `max_model_len=2048`
- `gpu_memory_utilization=0.6`
- `quantization=compressed-tensors`
- `kv_cache_dtype=fp8`

éªŒè¯ï¼š
```bash
# æ¨¡å‹åˆ—è¡¨
curl http://localhost:8000/v1/models

# æ–‡æœ¬ç”Ÿæˆ
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"Qwen3-Omni-AWQ-4bit","messages":[{"role":"user","content":"ä½ å¥½"}]}'
```

LLM æ€§èƒ½å‚è€ƒï¼ˆL40Sï¼‰ï¼šTTFT 0.078s, 171 tokens/s, VRAM â‰¤ 27 GiB (util=0.6)ã€‚

### 3.2 Bridge Demoï¼ˆLLM â†’ TTS æ¡¥æ¥ï¼‰

```bash
bash "/workspace/project 1/25/scripts/run_demo_bridge.sh"
```

Bridge ç­–ç•¥ï¼š
- ç”Ÿäº§è€…-æ¶ˆè´¹è€…å¹¶è¡Œï¼šLLM streaming ä¸ TTS å¹¶è¡Œ
- starter æ®µï¼š2~6 å­—ä¼˜å…ˆé€ TTS é™ä½é¦–åŒ…
- ä¸­æ–‡æ ‡ç‚¹ç«‹å³ flushï¼›æ— æ ‡ç‚¹æ—¶ 8~12 å­— flush
- æ”¯æŒ `/bridge/stop`ï¼ˆclient-side barge-inï¼‰

### 3.3 TTS Server

### æ–¹å¼ Aï¼šä½¿ç”¨è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
cd "/workspace/project 1/25"
bash scripts/run_tts_server.sh
```

è„šæœ¬å·²å†…ç½®é»„é‡‘åŸºçº¿é…ç½®ï¼ˆå« CUDA Graph CP=1, Decoder=1ï¼‰ã€‚éœ€è¦ä¿®æ”¹å‚æ•°æ—¶é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼š

```bash
# ä¾‹ï¼šå…³é—­ Decoder Graph åšå¯¹æ¯”å®éªŒ
TTS_DECODER_CUDAGRAPH=0 bash scripts/run_tts_server.sh
```

### æ–¹å¼ Bï¼šç›´æ¥å¯åŠ¨ï¼ˆè°ƒè¯•ç”¨ï¼‰

```bash
cd "/workspace/project 1/25/clients"
export PYTHONPATH="/workspace/vllm-omni"

# === å¿…é¡»å‚æ•° ===
export TTS_DEEP_STREAM_ENABLE=1
export TTS_DEEP_STREAM_PROCESS=0
export TTS_DEEP_STREAM_DECODE_MODE=incremental
export TTS_DEEP_STREAM_PACKET_TOKENS=2
export TTS_DEEP_STREAM_LEFT_CONTEXT=72
export TTS_DEEP_STREAM_DETERMINISTIC=1
export TTS_DEEP_STREAM_DETERMINISTIC_POLICY=greedy
export TTS_DEEP_STREAM_SEED_MODE=fixed
export TTS_DEEP_STREAM_SEED=42
export TTS_DEEP_STREAM_DEVICE=cuda:0
export TTS_DEEP_STREAM_CODEGEN_DEVICE=cuda:0
export TTS_DEEP_STREAM_MODEL_DIR="/workspace/models/Qwen3-TTS-12Hz-0.6B-CustomVoice"
export TTS_CODEGEN_GROUP_PARALLEL=0

python3 tts_server.py
```

### éªŒè¯æœåŠ¡å°±ç»ª

```bash
# ç­‰å¾… "Application startup complete"ï¼Œç„¶åï¼š
curl -s http://localhost:9000/tts/stream \
  -X POST -H "Content-Type: application/json" \
  -d '{"text":"ä½ å¥½ä¸–ç•Œ","speaker":"serena"}' \
  -o /dev/null -w '%{http_code}'
# åº”è¿”å› 200
```

### åœæ­¢æœåŠ¡

```bash
pkill -f "python3.*tts_server"
# ç¡®ä¿æ— æ®‹ç•™ï¼š
ps aux | grep tts_server
```

---

## 4. ç¯å¢ƒå˜é‡é€ŸæŸ¥è¡¨

### æ ¸å¿ƒé…ç½®ï¼ˆé“å¾‹â€”â€”ä¸å¯æ”¹å˜ï¼‰

| å˜é‡ | å€¼ | è¯´æ˜ |
|---|---|---|
| `TTS_CODEGEN_GROUP_PARALLEL` | `0` | **ç¦æ­¢æ”¹ä¸º auto**ï¼Œä¼šæ¯éŸ³è´¨ |
| `TTS_DEEP_STREAM_DECODE_MODE` | `incremental` | å¢é‡è§£ç æ¨¡å¼ |
| `TTS_DEEP_STREAM_PACKET_TOKENS` | `2` | æ¯åŒ… token æ•° |
| `TTS_DEEP_STREAM_LEFT_CONTEXT` | `72` | è§£ç å™¨å·¦ä¸Šä¸‹æ–‡ |
| `TTS_DEEP_STREAM_PROCESS` | `0` | åŒè¿›ç¨‹æ¨¡å¼ |
| `TTS_DEEP_STREAM_DETERMINISTIC` | `1` | ç¡®å®šæ€§æ¨¡å¼ |
| `TTS_DEEP_STREAM_DETERMINISTIC_POLICY` | `greedy` | è´ªå¿ƒé‡‡æ · |
| `TTS_DEEP_STREAM_SEED_MODE` | `fixed` | å›ºå®šç§å­ |
| `TTS_DEEP_STREAM_SEED` | `42` | ç§å­å€¼ |

### CUDA Graph å¼€å…³

| å˜é‡ | é»˜è®¤ | è¯´æ˜ |
|---|---|---|
| `TTS_CODEGEN_CUDAGRAPH_TALKER` | `0` | Talker CUDA Graphï¼ˆ**ä¿æŒ 0**ï¼Œæµ®ç‚¹ä¸ bit-exactï¼‰ |
| `TTS_CODEGEN_CUDAGRAPH_CP` | **`1`** | Code Predictor CUDA Graphï¼ˆ**é»„é‡‘é…ç½®é»˜è®¤å¼€å¯**ï¼‰ |
| `TTS_DECODER_CUDAGRAPH` | **`1`** | Decoder CUDA Graphï¼ˆ**é»„é‡‘é…ç½®é»˜è®¤å¼€å¯**ï¼‰ |

### è°ƒè¯•å¼€å…³

| å˜é‡ | é»˜è®¤ | è¯´æ˜ |
|---|---|---|
| `TTS_DEEP_STREAM_METRICS` | `0` | æ‰“å°é€åŒ…è€—æ—¶æŒ‡æ ‡ï¼ˆ**ä¼šå½±å“æ€§èƒ½**ï¼Œä»…è°ƒè¯•ç”¨ï¼‰ |
| `TTS_DEEP_STREAM_PACKET_TRACE` | `0` | æ‰“å°é€åŒ… trace |
| `TTS_CODE_DUMP_ENABLE` | `0` | ä¿å­˜ codes åˆ°ç£ç›˜ |
| `TTS_CODE_DUMP_DIR` | `output/code_dumps` | codes dump ç›®å½• |

### å›å½’/è´¨é‡

| å˜é‡ | è¯´æ˜ |
|---|---|
| `TTS_REGRESSION_BASELINE` | é»„é‡‘åŸºçº¿ summary.json è·¯å¾„ |
| `TTS_GATE_SNR_BASELINE_DB` | SNR è´¨é‡é—¨é™ï¼ˆé»˜è®¤ 15dBï¼‰ |
| `TTS_DEEP_STREAM_SILENCE_PACKETS` | é™éŸ³åŒ…æ£€æµ‹ï¼ˆå›å½’æ—¶è®¾ä¸º 0 é¿å…æ—¶é•¿åå·®ï¼‰ |
| `TTS_DEEP_STREAM_SILENCE_PACKETS_P1` | åŒä¸Š |
| `TTS_DEEP_STREAM_OFFLINE_FROM_CODES` | stream/offline ä¸€è‡´æ€§ï¼ˆå›å½’æ—¶è®¾ä¸º 1ï¼‰ |

---

## 5. å›å½’æµ‹è¯•

### 5.1 é»„é‡‘åŸºçº¿

å½“å‰é»„é‡‘åŸºçº¿ï¼š`/workspace/project 1/25/output/regression/20260208_200725/summary.json`

åŸºçº¿é…ç½®ï¼š0.6B æ¨¡å‹ï¼ŒGP=0ï¼Œincrementalï¼Œpacket=2ï¼Œleft_context=72ï¼Œgreedy seed=42ï¼Œ**CP Graph=1ï¼ŒDecoder Graph=1**ã€‚

> å†å²åŸºçº¿ï¼ˆæ—  Graphï¼‰ï¼š`output/regression/20260207_192126/`ï¼Œä¿ç•™ä¾›å‚è€ƒã€‚

### 5.2 è¿è¡Œå›å½’

```bash
# Fastï¼ˆçº¦ 2-3 åˆ†é’Ÿï¼Œä¸ä¿å­˜ wavï¼‰
bash "/workspace/project 1/25/scripts/run_ci_regression.sh" --mode fast

# Fullï¼ˆçº¦ 10 åˆ†é’Ÿï¼Œä¿å­˜ wav ä¾›è¯•å¬ï¼‰
bash "/workspace/project 1/25/scripts/run_ci_regression.sh" --mode full
```

### 5.3 è´¨é‡ Gatesï¼ˆæ‰€æœ‰ gate å¿…é¡» PASSï¼‰

| Gate | è¯´æ˜ | é˜ˆå€¼ |
|---|---|---|
| `determinism` | å¤šæ¬¡è¿è¡Œ hash ä¸€è‡´ | hash_unique=1 |
| `abs_duration_diff_ms` | stream vs offline æ—¶é•¿å·® | â‰¤ 500ms |
| `repeat` | æ— é‡å¤ç‰‡æ®µ | 0 |
| `SNR_vs_baseline` | ä¸é»„é‡‘åŸºçº¿ä¿¡å™ªæ¯” | â‰¥ 15dB |
| `TTFA` | é¦–éŸ³é¢‘åŒ…å»¶è¿Ÿ | â‰¤ 350ms |
| `stream_bad_audio` | æ— ç©º/æŸåéŸ³é¢‘ | empty |

### 5.4 å…¸å‹å›å½’æµç¨‹

```bash
# 1. å¯åŠ¨ serverï¼ˆé»„é‡‘é…ç½®å·²å†…ç½® CUDA Graphï¼‰
bash scripts/run_tts_server.sh &

# 2. ç­‰å¾…å°±ç»ªï¼ˆçœ‹åˆ° "Application startup complete"ï¼‰

# 3. è·‘ fast regression
bash scripts/run_ci_regression.sh --mode fast

# 4. ç¡®è®¤ PASS åè·‘ full
SAVE_WAV=1 bash scripts/run_ci_regression.sh --mode full

# 5. æ£€æŸ¥äº§ç‰©
ls output/regression/latest/
# summary.json, summary_brief.json, *.wav
```

---

## 6. CUDA Graph åŠ é€Ÿ

### 6.1 åŸç†

TTS æ¨ç†ç“¶é¢ˆæ˜¯ **CPU ç«¯ cudaLaunchKernel è°ƒç”¨**ï¼ˆå  CPU æ—¶é—´ 91%+ï¼‰ã€‚CUDA Graph å°†ä¸€ç³»åˆ— GPU æ“ä½œ"å½•åˆ¶"ä¸ºå›¾ï¼Œåç»­åªéœ€ä¸€æ¬¡ graph launch å³å¯é‡æ”¾æ‰€æœ‰æ“ä½œã€‚

### 6.2 Code Predictor Graphï¼ˆæ¨èå¼€å¯ï¼‰

```bash
export TTS_CODEGEN_CUDAGRAPH_CP=1
```

- 14 ä¸ª lm_head å„ä¸€ä¸ª graphï¼Œå…±äº«ä¸€ä¸ª frozen cache
- ä»… decode stepï¼ˆq_len=1ï¼‰èµ° graphï¼›prefill ä¿æŒ eager
- å½¢çŠ¶ä¸åŒ¹é…è‡ªåŠ¨ fallback eager

**æ•ˆæœ**ï¼šcodegen-only RTF ä» 0.89 é™è‡³ 0.45ï¼ˆ1.97x åŠ é€Ÿï¼‰ï¼Œ100% bit-exactã€‚

### 6.3 Decoder Graphï¼ˆæ¨èå¼€å¯ï¼‰

```bash
export TTS_DECODER_CUDAGRAPH=1
```

- å¯¹ conv/upsample è·¯å¾„åš graph capture
- æœåŠ¡å¯åŠ¨æ—¶é¢„æ•è·ï¼ˆpre-captureï¼‰ï¼Œé¿å…è¿è¡Œæ—¶ `cudaErrorStreamCaptureUnsupported`
- ä½¿ç”¨ä¸“ç”¨ CUDA stream éš”ç¦» codegen å’Œ decode
- `kernel_size=1` çš„å·ç§¯è·³è¿‡ state æ”¶é›†

**æ•ˆæœ**ï¼še2e RTF P50 ä» ~0.93 é™è‡³ ~0.70ã€‚

### 6.4 Talker Graphï¼ˆä¸æ¨èï¼‰

```bash
export TTS_CODEGEN_CUDAGRAPH_TALKER=0  # ä¿æŒå…³é—­
```

ç”±äº full-buffer attention çš„æµ®ç‚¹ç²¾åº¦å·®å¼‚ï¼ŒTalker Graph æ— æ³•å®ç° bit-exactã€‚

### 6.5 CUDA Graph è°ƒè¯•è¦ç‚¹

1. **å¿…é¡»ç”¨ `torch.no_grad()`**ï¼Œä¸èƒ½ç”¨ `torch.inference_mode()`ï¼ˆåè€…ä¼šæŠ¥ "Inference tensors cannot be saved for backward"ï¼‰

2. **graph capture å‰å¿…é¡»åš warmup run**ï¼Œå¦åˆ™ cuDNN/cuBLAS workspace æœªåˆå§‹åŒ–ï¼Œè¾“å‡ºå…¨é›¶

3. **capture åè¿”å› replay çš„è¾“å‡º**ï¼Œä¸èƒ½è¿”å› capture-time çš„è¾“å‡ºï¼ˆcapture-time è¾“å‡ºåªæ˜¯å ä½ç¬¦ï¼‰

4. **CUDA Graph capture æ˜¯è¿›ç¨‹çº§å…¨å±€çŠ¶æ€**ï¼Œåœ¨ `_init_deep_stream_backend` å¯åŠ¨æ—¶é¢„æ•è·ï¼Œä¸èƒ½åœ¨è¯·æ±‚å¤„ç†æ—¶æ•è·

5. **DecoderGraphAccelerator å¿…é¡»å…¨å±€ç¼“å­˜**ï¼Œæ¯ä¸ªè¯·æ±‚å¤ç”¨åŒä¸€å®ä¾‹ï¼Œå¦åˆ™æ¯æ¬¡é‡æ–° capture å¯¼è‡´ TTFA æš´å¢

6. **é™æ€ buffer + copy_()**ï¼šæ‰€æœ‰åŠ¨æ€è¾“å…¥å¿…é¡»å…ˆ `copy_()` åˆ°é¢„åˆ†é…çš„é™æ€ tensorï¼Œå† `graph.replay()`

7. **ä¸“ç”¨ CUDA stream**ï¼šdecoder graph éœ€è¦ä¸“ç”¨ streamï¼Œé¿å…ä¸ codegen çš„é»˜è®¤ stream å†²çª
   ```python
   self._stream = torch.cuda.Stream()
   self._stream.wait_stream(torch.cuda.current_stream())  # ç­‰è¾“å…¥å°±ç»ª
   with torch.cuda.stream(self._stream):
       self.graph.replay()
   torch.cuda.current_stream().wait_stream(self._stream)  # ç­‰ç»“æœå°±ç»ª
   ```

8. **state buffer åªæ”¶é›† kernel_size > 1 çš„å·ç§¯**ï¼Œkernel_size=1 çš„å·ç§¯æ— çŠ¶æ€ï¼Œæ”¶é›†ä¼šæŠ¥é”™

### 6.6 CUDA Graph Fallback

å½“è¾“å…¥ shape ä¸ capture æ—¶ä¸åŒ¹é…ï¼Œè‡ªåŠ¨ fallback eager å¹¶è®°å½•åŸå› ï¼š

```python
# Server meta è¾“å‡ºï¼š
{
  "cudagraph_cp_used": true,
  "decoder_graph_stats": {
    "graph_steps": 120,
    "eager_steps": 1,          # step 0 æ°¸è¿œæ˜¯ eager
    "fallback_count": 0,
    "fallback_reasons_topk": {}
  }
}
```

---

## 7. å…³é”®æŒ‡æ ‡å®šä¹‰

| æŒ‡æ ‡ | å®šä¹‰ | ç›®æ ‡ |
|---|---|---|
| **RTF** (Real-Time Factor) | `wall_time / audio_duration` | < 0.7 |
| **TTFA** (Time to First Audio) | ä»è¯·æ±‚åˆ°ç¬¬ä¸€ä¸ªéŸ³é¢‘ chunk è¾“å‡º | â‰¤ 350ms |
| **codegen_iter_wall_ms** | ç­‰å¾… `next(codes_iter)` çš„æ—¶é—´ | â€” |
| **decode_wall_total_ms** | decoder æ€»è€—æ—¶ | â€” |
| **glue_wall_total_ms** | å½’ä¸€åŒ–/è®¾å¤‡ä¼ è¾“/PCM åˆ†å—ç­‰æ‚é¡¹ | â€” |
| **SNR** | stream vs offline/baseline ä¿¡å™ªæ¯” | â‰¥ 15dB |
| **launches/step** | æ¯æ­¥ cudaLaunchKernel è°ƒç”¨æ•° | è¶Šå°‘è¶Šå¥½ |

### æ€§èƒ½åŸºçº¿ï¼ˆL40S, 0.6B æ¨¡å‹ï¼‰

| é…ç½® | RTF P50 | RTF P95 | TTFA P95 | çŠ¶æ€ |
|---|---|---|---|---|
| Baseline (no graph) | 0.93 | 0.98 | 330ms | å†å²å‚è€ƒ |
| CP-only graph | 0.93 | 0.93 | 247ms | â€” |
| **CP + Decoder graph** âœ… | **0.70** | **0.76** | **244ms** | **å½“å‰é»„é‡‘é…ç½®** |

> é»„é‡‘åŸºçº¿äº§ç‰©ï¼š`output/regression/20260208_200725/`ï¼ˆFull regression, 10-run determinism, SNR 120dB, å…¨ gate PASSï¼‰

---

## 8. å¸¸è§é”™è¯¯ & ä¿®å¤

### å¯åŠ¨ç±»

| é”™è¯¯ | åŸå›  | ä¿®å¤ |
|---|---|---|
| `KeyError: 'qwen3_tts'` | æ¨¡å‹æœªæ³¨å†Œ | ç¡®ä¿ç”¨ `Qwen3TTSModel.from_pretrained` åŠ è½½ |
| `ValueError: Unsupported speakers` | speaker åé”™è¯¯ | ä½¿ç”¨ `serena`ï¼ˆä¸æ˜¯ `Chelsie`ï¼‰ |
| `GPU free < 8000 MiB; aborting` | æ˜¾å­˜ä¸è¶³ | `pkill -f python3` æ¸…ç†æ®‹ç•™è¿›ç¨‹ |
| ç«¯å£å·²å ç”¨ | ä¸Šæ¬¡è¿›ç¨‹æœªå½»åº•é€€å‡º | `pkill -f tts_server` |

### CUDA Graph ç±»

| é”™è¯¯ | åŸå›  | ä¿®å¤ |
|---|---|---|
| `Inference tensors cannot be saved for backward` | ç”¨äº† `inference_mode` | æ”¹ç”¨ `torch.no_grad()` |
| `cudaErrorStreamCaptureUnsupported` | è¯·æ±‚å¤„ç†æ—¶ capture | å¯åŠ¨æ—¶é¢„æ•è· |
| Graph è¾“å‡ºå…¨é›¶ | æ²¡åš warmup | capture å‰åŠ  warmup run |
| Graph è¾“å‡ºä¸ bit-exact | capture-time è¾“å‡º vs replay è¾“å‡º | è¿”å› replay åçš„ `static_out` |
| `operation not permitted when stream is capturing` | é»˜è®¤ stream ä¸Šæœ‰å¹¶å‘æ“ä½œ | ç”¨ä¸“ç”¨ `torch.cuda.Stream()` |
| `IndexError: index 15 is out of range` (CP) | `n_q` å€¼é”™è¯¯ | ç”¨ `decoder.quantizer.max_n_q`ï¼ˆ=16ï¼‰ |

### å›å½’ç±»

| é”™è¯¯ | åŸå›  | ä¿®å¤ |
|---|---|---|
| `stream_bad_audio=empty` | æ²¡è¯»å– stream æ•°æ® | ç¡®ä¿ `iter_content` åœ¨ `if save_wav` å¤– |
| `baseline_missing_or_no_cases` | åŸºçº¿è·¯å¾„æ— æ•ˆ | æ£€æŸ¥ `TTS_REGRESSION_BASELINE` è·¯å¾„ |
| æ—¶é•¿åå·®å¤§ | é™éŸ³æå‰æˆªæ–­ | è®¾ `SILENCE_PACKETS=0` |
| SNR è´Ÿå€¼ | stream ä¸ offline è¾“å‡ºä¸ä¸€è‡´ | æ£€æŸ¥ `OFFLINE_FROM_CODES=1` |

---

## 9. è¸©å‘ç»éªŒ & é“å¾‹

### ğŸš« é“å¾‹ï¼ˆè¿åå¿…å‡º bugï¼‰

1. **ç¦æ­¢ `GROUP_PARALLEL=auto`**ï¼šä¼šå¯¼è‡´ predictor å¹¶è¡Œæ—¶éŸ³è´¨å´©æºƒï¼ŒSNR æš´è·Œ
2. **ç¦æ­¢ `TTS_CODEGEN_CUDAGRAPH_TALKER=1`**ï¼šæµ®ç‚¹ç²¾åº¦ä¸ä¸€è‡´ï¼Œæ— æ³• bit-exact
3. **å›å½’æ—¶å¿…é¡»å…³é—­ `SILENCE_PACKETS`**ï¼šå¦åˆ™ç”Ÿæˆçš„éŸ³é¢‘è¢«æˆªçŸ­ï¼Œabs_duration_diff æš´å¢
4. **Graph capture å¿…é¡»åœ¨ `torch.no_grad()` ä¸‹**ï¼šä¸è¦ç”¨ `inference_mode()`
5. **DecoderGraphAccelerator å¿…é¡»å…¨å±€å•ä¾‹**ï¼šä¸è¦æ¯ä¸ªè¯·æ±‚é‡å»º

### ğŸ’¡ ç»éªŒ

1. **codegen æ˜¯ååç“¶é¢ˆ**ï¼š91%+ CPU æ—¶é—´èŠ±åœ¨ `cudaLaunchKernel`ï¼Œä¸æ˜¯ GPU è®¡ç®—
2. **decode ä¸­ conv/upsample å  90%+**ï¼Œpre_transformer åªå çº¦ 10%
3. **`torch.cuda.synchronize()` ä¼šä¸¥é‡å¹²æ‰°è®¡æ—¶**ï¼šå¦‚éœ€æ‹†åˆ†è®¡æ—¶ï¼Œç”¨ CUDA event æˆ– `time.monotonic()`
4. **vLLM-Omni åªæä¾›æ¨¡å‹åŠ è½½**ï¼šå®é™…æ¨ç†èµ° HuggingFace `generate()`ï¼Œä¸èµ° vLLM å¼•æ“
5. **æ€ server åè¦ç¡®è®¤è¿›ç¨‹å½»åº•é€€å‡º**ï¼švLLM worker å¯èƒ½å˜æˆåƒµå°¸è¿›ç¨‹å æ˜¾å­˜
6. **é¦–æ¬¡è¯·æ±‚ TTFA åé«˜**ï¼šcuDNN å’Œ CUDA runtime éœ€è¦åˆå§‹åŒ–ï¼Œç¬¬ 2 æ¬¡å¼€å§‹æ‰æ˜¯ç¨³æ€
7. **CP Graph çš„ 14 ä¸ª lm_head å¿…é¡»å…±äº«åŒä¸€ä¸ª frozen cache**ï¼šåˆ†å¼€çš„ cache ä¼šå¯¼è‡´ä¸ bit-exact
8. **DynamicCache çš„ `get_seq_length()` è¿”å› `MAX_SEQ_LEN`**ï¼šéœ€è¦ monkey-patch è¿”å› `_seen_tokens`

### ğŸ” è°ƒè¯•æŠ€å·§

1. **å¿«é€ŸéªŒè¯ç¡®å®šæ€§**ï¼šåŒä¸€è¾“å…¥è·‘ 3 æ¬¡ï¼Œæ¯”è¾ƒ PCM hash æ˜¯å¦ä¸€è‡´
2. **ç”¨ `nsys profile` æŠ“ kernel launch**ï¼š`nsys profile -t cuda python3 your_script.py`
3. **ç”¨ code_dump ä¿å­˜ codes**ï¼š`TTS_CODE_DUMP_ENABLE=1`ï¼Œç„¶åå¯ä»¥ç¦»çº¿é‡æ”¾ decoder
4. **decoder-only microbench**ï¼š`python3 decoder_microbench.py` å¯ä»¥éš”ç¦»æµ‹è¯• decoder æ€§èƒ½
5. **codegen-only benchmark**ï¼š`python3 codegen_only_benchmark.py` å¯ä»¥éš”ç¦»æµ‹è¯• codegen æ€§èƒ½
6. **throughput benchmark**ï¼š`python3 throughput_benchmark.py --server http://localhost:9000 --duration 30`

---

## 10. æ–‡ä»¶ç´¢å¼•

### æ ¸å¿ƒæœåŠ¡

| æ–‡ä»¶ | è¯´æ˜ |
|---|---|
| `clients/tts_server.py` | TTS ä¸»æœåŠ¡ï¼ˆFastAPIï¼‰ï¼ŒåŒ…å« `/tts/stream` å’Œ `/synthesize` |
| `clients/tts_incremental_decoder.py` | å¢é‡è§£ç å™¨ï¼Œæµå¼ conv/upsample |
| `clients/codegen_cudagraph.py` | Codegen CUDA Graphï¼ˆTalker + CPï¼‰ |
| `clients/decoder_cudagraph.py` | Decoder CUDA Graph |

### æµ‹è¯• & åŸºå‡†

| æ–‡ä»¶ | è¯´æ˜ |
|---|---|
| `clients/tts_regression_suite.py` | å›å½’æµ‹è¯•å¥—ä»¶ï¼ˆfast/fullï¼‰ |
| `clients/codegen_only_benchmark.py` | Codegen-only RTF åŸºå‡† |
| `clients/decoder_microbench.py` | Decoder-only å¾®åŸºå‡† |
| `clients/throughput_benchmark.py` | ç«¯åˆ°ç«¯åååŸºå‡† |
| `clients/tts_codes_dump.py` | Codes dump å·¥å…· |
| `clients/tts_codes_eval.py` | Codes è´¨é‡è¯„ä¼° |
| `clients/tts_cancel_stress.py` | å–æ¶ˆè¯·æ±‚å‹æµ‹ |
| `clients/tts_cached_decode_poc.py` | ç¼“å­˜è§£ç  PoCï¼ˆå‚è€ƒç”¨ï¼‰ |
| `clients/llm_smoke_test.py` | LLM çƒŸæµ‹ |

### Voice Agent è¿è¡Œæ—¶ (D1â€“D5 æ–°å¢)

| æ–‡ä»¶ | è¯´æ˜ |
|---|---|
| `runtime/livekit_agent.py` | **LiveKit Voice Agent** â€” VADâ†’STT(Omni)â†’LLM(Omni)â†’TTS, å« TraceCollector 9ç‚¹æ‰“ç‚¹ |
| `runtime/token_server.py` | JWT Token API (:3000) + å‰ç«¯é™æ€æ–‡ä»¶æ‰˜ç®¡ |
| `runtime/webrtc_test.html` | WebRTC å‰ç«¯ UIï¼ˆæµè§ˆå™¨ç«¯ EoT æ£€æµ‹ + P50/P95 ç»Ÿè®¡ï¼‰ |
| `runtime/duplex_controller.py` | åŒå·¥çŠ¶æ€æœºï¼ˆLISTENING/THINKING/SPEAKING/INTERRUPTINGï¼‰+ çº§è” cancel |
| `runtime/gpu_scheduler.py` | GPU ç¡¬ä¼˜å…ˆçº§è°ƒåº¦å™¨ï¼ˆfast lane æŠ¢å , slow lane try_acquireï¼‰ |
| `runtime/vad_silero.py` | Silero VAD å°è£…ï¼ˆCPU, 512 samples @16kHzï¼‰ |
| `runtime/live_duplex.py` | æ¨¡æ‹Ÿ live å¯¹è¯ä¼šè¯ï¼ˆç”¨ WAV æ–‡ä»¶æ¨¡æ‹Ÿéº¦å…‹é£ï¼‰ |

### è„šæœ¬

| æ–‡ä»¶ | è¯´æ˜ |
|---|---|
| `scripts/run_tts_server.sh` | å¯åŠ¨ TTS æœåŠ¡ï¼ˆé»„é‡‘é…ç½® + CUDA Graph + auto-restartï¼‰ |
| `scripts/run_ci_regression.sh` | è¿è¡Œ CI å›å½’ï¼ˆ`--mode fast/full`ï¼‰ |
| `scripts/run_llm_server.sh` | å¯åŠ¨ LLM æœåŠ¡ï¼ˆvLLM OpenAI APIï¼‰ |
| `scripts/start_all.sh` | **ä¸€é”®ç®¡ç†** `{start\|restart\|stop\|status}` æ‰€æœ‰æœåŠ¡ |
| `scripts/supervisor_voice_agent.conf` | Supervisor è¿›ç¨‹ç®¡ç†é…ç½®ï¼ˆå¤‡ç”¨ï¼‰ |
| `scripts/setup_tts_env.sh` | TTS ç¯å¢ƒåˆå§‹åŒ– |
| `scripts/setup_llm_env.sh` | LLM ç¯å¢ƒåˆå§‹åŒ– |
| `/post_start.sh` | RunPod Pod é‡å¯åè‡ªåŠ¨æ¢å¤æ‰€æœ‰æœåŠ¡ |

### é…ç½®

| æ–‡ä»¶ | è¯´æ˜ |
|---|---|
| `clients/texts_p0_base.json` | æµ‹è¯•æ–‡æœ¬é›†ï¼ˆå« short_01, long_03 ç­‰ï¼‰ |
| `clients/voices_base.json` | æµ‹è¯•è¯­éŸ³é…ç½® |
| `artifacts/qwen3_tts_l40s.yaml` | L40S ä½æ˜¾å­˜é…ç½® |

### è¾“å‡º

| ç›®å½• | è¯´æ˜ |
|---|---|
| `output/regression/20260208_200725/` | **å½“å‰é»„é‡‘åŸºçº¿**ï¼ˆCP+Decoder Graph, å…¨ PASSï¼‰ |
| `output/regression/latest/` | æœ€æ–°å›å½’çš„ç¬¦å·é“¾æ¥ |
| `output/day5_e2e_traces.jsonl` | D5 ç«¯åˆ°ç«¯å»¶è¿Ÿ traceï¼ˆ22 è½®ï¼‰ |
| `output/day3_stress_cancel_report.json` | D3 å‹æµ‹æŠ¥å‘Šï¼ˆ200è½®, cancel P95=7.5msï¼‰ |
| `output/day3_vad_eval.json` | D3 VAD è¯„ä¼°ç»“æœ |

### Voice Agent ç¯å¢ƒå˜é‡é€ŸæŸ¥

| å˜é‡ | é»˜è®¤å€¼ | è¯´æ˜ |
|---|---|---|
| `VAD_SILENCE_MS` | 200 | VAD hangoverï¼ˆé™éŸ³åˆ¤å®šï¼‰ï¼Œè¶Šå°è¶Šçµæ• |
| `TTS_FRAME_MS` | 20 | TTS å‘å¸ƒå¸§ç²’åº¦ ms |
| `MIN_ENDPOINTING` | 0.3 | LiveKit æœ€å° endpointing delay |
| `ENABLE_CONTINUATION` | 1 | LLM å»¶ç»­å¥æœºåˆ¶ï¼ˆå…ˆçŸ­åé•¿ï¼‰ |
| `LLM_MAX_TOKENS` | 150 | LLM æœ€å¤§ token æ•° |
| `LLM_TEMPERATURE` | 0.3 | LLM æ¸©åº¦ |
| `LIVEKIT_URL` | wss://...livekit.cloud | LiveKit Cloud åœ°å€ |
| `LIVEKIT_API_KEY` | â€” | LiveKit API Key |
| `LIVEKIT_API_SECRET` | â€” | LiveKit API Secret |

### Voice Agent å¿«é€Ÿå¯åŠ¨

```bash
# 1. ç¡®ä¿ LLM + TTS å·²è¿è¡Œ
bash scripts/start_all.sh status

# 2. å¯åŠ¨å…¨éƒ¨æœåŠ¡ï¼ˆå« Agent + Token Serverï¼‰
bash scripts/start_all.sh start

# 3. æµè§ˆå™¨è®¿é—®ï¼ˆé€šè¿‡ Jupyter proxyï¼‰
# https://POD_ID-8888.proxy.runpod.net/proxy/3000/?token=JUPYTER_TOKEN

# 4. æŸ¥çœ‹å»¶è¿Ÿ trace
cat output/day5_e2e_traces.jsonl | python3 -m json.tool

# 5. é‡å¯ Agentï¼ˆæ”¹ä»£ç åï¼‰
bash scripts/start_all.sh restart
```

### LiveKit Agent è¸©å‘ç»éªŒï¼ˆv1.4 APIï¼‰

| å‘ | è§£å†³æ–¹æ¡ˆ |
|---|---|
| `JobContext` æ²¡æœ‰ `participant` å±æ€§ | ç”¨ `ctx.connect()` + `ctx.wait_for_participant()` |
| `AgentSession.start()` ä¸æ¥å— `participant` | åªä¼  `agent` å’Œ `room` |
| `LLMStream.__init__()` ç¼ºå‚æ•° | å¿…é¡»ä¼  `tools=[]` å’Œ `conn_options` |
| `ChatChunk` ç¼º `id` å­—æ®µ | å¿…é¡»ä¼  `id="omni"` |
| `ChunkedStream._run()` ç­¾åå˜åŒ– | å¿…é¡»æ¥å— `output_emitter` å‚æ•° |
| `AudioEmitter isn't started` | **å¿…é¡»åœ¨ `_run()` å¼€å¤´å°±è°ƒ `initialize()`**ï¼Œå³ä½¿æ²¡éŸ³é¢‘ä¹Ÿæ¨é™éŸ³å¸§ |
| `start_segment()` ä»…é™ stream=True | éæµå¼ ChunkedStream ä¸ç”¨ segment ç®¡ç† |
| Omni audio æ ¼å¼ | ç”¨ `{"type": "audio_url", "audio_url": {"url": "data:audio/wav;base64,..."}}`ï¼Œä¸æ˜¯ `input_audio` |
| åŒæ­¥ HTTP é˜»å¡äº‹ä»¶å¾ªç¯ | æ‰€æœ‰ requests.post å¿…é¡» `run_in_executor()` |
| éŸ³é¢‘ç¼–ç é˜»å¡ä¸»çº¿ç¨‹ | base64 ç¼–ç ä¹Ÿè¦ offload åˆ°çº¿ç¨‹ |
