<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Qwen3 è¯­éŸ³åŠ©æ‰‹</title>
  <script src="https://cdn.jsdelivr.net/npm/livekit-client@2.9.1/dist/livekit-client.umd.js"></script>
  <style>
    * { margin: 0; padding: 0; box-sizing: border-box; }
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      background: linear-gradient(135deg, #0f0f23 0%, #1a1a3e 50%, #0f0f23 100%);
      color: #e0e0e0;
      min-height: 100vh;
      display: flex;
      flex-direction: column;
      align-items: center;
      padding: 20px;
    }
    h1 {
      font-size: 1.6em;
      margin: 20px 0 10px;
      background: linear-gradient(90deg, #6366f1, #8b5cf6);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
    }
    .subtitle { color: #888; font-size: 0.85em; margin-bottom: 20px; }
    .status {
      padding: 8px 20px;
      border-radius: 20px;
      font-size: 0.9em;
      font-weight: 500;
      margin-bottom: 15px;
      transition: all 0.3s;
    }
    .status.disconnected { background: rgba(239,68,68,0.15); color: #ef4444; border: 1px solid rgba(239,68,68,0.3); }
    .status.connecting { background: rgba(234,179,8,0.15); color: #eab308; border: 1px solid rgba(234,179,8,0.3); }
    .status.connected { background: rgba(34,197,94,0.15); color: #22c55e; border: 1px solid rgba(34,197,94,0.3); }
    .status.error { background: rgba(239,68,68,0.15); color: #ef4444; border: 1px solid rgba(239,68,68,0.3); }

    .controls {
      display: flex;
      gap: 10px;
      margin-bottom: 20px;
    }
    button {
      padding: 10px 28px;
      border: none;
      border-radius: 10px;
      font-size: 1em;
      cursor: pointer;
      transition: all 0.2s;
      font-weight: 500;
    }
    #connectBtn {
      background: linear-gradient(135deg, #6366f1, #8b5cf6);
      color: white;
      box-shadow: 0 4px 15px rgba(99,102,241,0.3);
    }
    #connectBtn:hover:not(:disabled) { transform: translateY(-1px); box-shadow: 0 6px 20px rgba(99,102,241,0.4); }
    #connectBtn:disabled { opacity: 0.5; cursor: not-allowed; }
    #disconnectBtn {
      background: rgba(239,68,68,0.15);
      color: #ef4444;
      border: 1px solid rgba(239,68,68,0.3);
    }
    #disconnectBtn:hover:not(:disabled) { background: rgba(239,68,68,0.25); }
    #disconnectBtn:disabled { opacity: 0.3; cursor: not-allowed; }

    .visualizer {
      width: 200px;
      height: 200px;
      border-radius: 50%;
      background: radial-gradient(circle, rgba(99,102,241,0.1) 0%, transparent 70%);
      border: 2px solid rgba(99,102,241,0.2);
      display: flex;
      align-items: center;
      justify-content: center;
      margin: 15px 0;
      transition: all 0.3s;
    }
    .visualizer.active {
      border-color: rgba(99,102,241,0.6);
      box-shadow: 0 0 40px rgba(99,102,241,0.2);
      animation: pulse 2s ease-in-out infinite;
    }
    .visualizer.speaking {
      border-color: rgba(34,197,94,0.6);
      box-shadow: 0 0 40px rgba(34,197,94,0.2);
      animation: pulse-green 1s ease-in-out infinite;
    }
    @keyframes pulse {
      0%, 100% { transform: scale(1); }
      50% { transform: scale(1.03); }
    }
    @keyframes pulse-green {
      0%, 100% { transform: scale(1); }
      50% { transform: scale(1.05); }
    }
    .visualizer-text {
      font-size: 1.1em;
      color: #888;
    }

    .log-container {
      width: 100%;
      max-width: 600px;
      margin-top: 15px;
    }
    .log-header {
      display: flex;
      justify-content: space-between;
      align-items: center;
      margin-bottom: 8px;
    }
    .log-header h3 { font-size: 0.9em; color: #888; }
    .log-header button {
      padding: 4px 12px;
      font-size: 0.75em;
      background: rgba(255,255,255,0.05);
      color: #888;
      border: 1px solid rgba(255,255,255,0.1);
      border-radius: 6px;
    }
    #log {
      background: rgba(0,0,0,0.3);
      border: 1px solid rgba(255,255,255,0.06);
      border-radius: 12px;
      padding: 12px;
      height: 200px;
      overflow-y: auto;
      font-family: 'SF Mono', Monaco, monospace;
      font-size: 0.8em;
      line-height: 1.6;
    }
    .log-entry.info { color: #94a3b8; }
    .log-entry.event { color: #6366f1; }
    .log-entry.success { color: #22c55e; }
    .log-entry.error { color: #ef4444; }
    .log-entry.audio { color: #eab308; }
  </style>
</head>
<body>
  <h1>ğŸ™ï¸ Qwen3 è¯­éŸ³åŠ©æ‰‹</h1>
  <p class="subtitle">WebRTC å®æ—¶è¯­éŸ³é€šè¯ Â· Omni + TTS</p>

  <div id="status" class="status disconnected">æœªè¿æ¥</div>
  <div id="stats" style="color:#6366f1; font-size:0.85em; margin:5px 0; font-family:monospace;"></div>

  <div class="controls">
    <button id="connectBtn" onclick="connect()">ğŸ”Œ è¿æ¥</button>
    <button id="disconnectBtn" onclick="disconnect()" disabled>æ–­å¼€</button>
  </div>

  <div id="visualizer" class="visualizer">
    <span class="visualizer-text">ğŸ¤ ç­‰å¾…è¿æ¥</span>
  </div>

  <div class="log-container">
    <div class="log-header">
      <h3>æ—¥å¿—</h3>
      <button onclick="document.getElementById('log').innerHTML=''">æ¸…ç©º</button>
    </div>
    <div id="log"></div>
  </div>

  <script>
    let room = null;
    window.room = null;  // D12: expose for Playwright access
    let agentAudioEl = null;

    // â”€â”€ D12 AutoBrowser: URL å‚æ•°è§£æ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const _params = new URLSearchParams(window.location.search);
    const AUTO_MODE       = _params.get('auto') === '1';
    const PARAM_TRACE_ID  = _params.get('trace_id') || '';
    const PARAM_CASE_ID   = _params.get('case_id') || '';
    const PARAM_TURN_ID   = _params.get('turn_id') || '0';
    const PARAM_ROOM      = _params.get('room') || '';
    const PARAM_IDENTITY  = _params.get('identity') || '';
    const PARAM_TOKEN     = _params.get('lk_token') || '';
    const PARAM_LK_URL    = _params.get('lk_url') || '';
    const PARAM_CALLBACK  = _params.get('callback_url') || '';
    const AUTO_DISCONNECT_S = parseInt(_params.get('auto_disconnect_s') || '30', 10);

    // â”€â”€ D12: Exposed state for Playwright â”€â”€
    window.__autobrowser_traces = [];
    window.__autobrowser_done = false;
    window.__autobrowser_joined = false;
    window.__autobrowser_error = null;
    window.__autobrowser_reply_audio_chunks = [];  // base64-encoded WebM chunks
    window.__autobrowser_recording_blob = null;    // final Blob

    // â”€â”€ Browser trace state â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const browserTraces = [];
    let currentTrace = null;
    let audioContext = null;
    let micAnalyser = null;
    let agentAnalyser = null;
    let agentAudioSource = null;
    let speechDetected = false;
    let speechEndTimer = null;
    let agentPlayoutDetected = false;
    let mediaRecorder = null;
    let recordedChunks = [];

    const SPEECH_THRESHOLD   = 0.015;   // user mic energy threshold
    const SILENCE_TIMEOUT_MS = 400;     // user silence â†’ EoT
    const PLAYOUT_THRESHOLD  = 0.008;   // agent playout energy threshold (lower = more sensitive)

    function startBrowserTrace() {
      currentTrace = {
        trace_id: PARAM_TRACE_ID,
        case_id: PARAM_CASE_ID,
        turn_id: PARAM_TURN_ID,
        t_user_speech_start: null,
        t_user_speech_end: null,
        t_user_eot_browser: null,
        t_agent_track_first_frame_recv: null,
        t_browser_first_playout: null,
        t_first_audio_playout: null,  // compat alias
      };
      speechDetected = false;
      agentPlayoutDetected = false;
    }

    // D12: Callable from Playwright after mic mute.
    // The mic-mute moment IS the user EoT (since fake audio just stopped).
    // We reset traces and record t_user_eot_browser = now, then wait for
    // the next agent playout to compute USER_KPI.
    window.resetForMeasurement = function() {
      browserTraces.length = 0;
      window.__autobrowser_traces = [];
      agentPlayoutDetected = false;
      // Start fresh trace with EoT = now (mic just muted)
      startBrowserTrace();
      const eotNow = performance.now();
      currentTrace.t_user_speech_start = eotNow - 100;  // nominal
      currentTrace.t_user_speech_end = eotNow;
      currentTrace.t_user_eot_browser = eotNow;
      speechDetected = false;
      console.log('[autobrowser:event] Trace reset: EoT set to mic-mute time');

      // If agent track is already subscribed, its next audio frame is the playout.
      // Use a short delay to let the system settle, then check for playout.
      if (agentAudioEl || agentAnalyser) {
        // Agent track already exists â€” schedule playout detection
        setTimeout(() => {
          if (currentTrace && !currentTrace.t_browser_first_playout) {
            currentTrace.t_browser_first_playout = performance.now();
            currentTrace.t_first_audio_playout = currentTrace.t_browser_first_playout;
            console.log('[autobrowser:event] Post-mute playout detected (agent already connected)');
            finalizeTrace();
          }
        }, 200);  // 200ms: accounts for any buffered agent audio arriving
      }
    };

    // â”€â”€ Mic energy monitor (User EoT detection) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // NOTE: uses setInterval (not requestAnimationFrame) for headless Chromium compat
    let micMonitorInterval = null;
    function monitorMic(stream) {
      if (!audioContext) audioContext = new (window.AudioContext || window.webkitAudioContext)();
      if (audioContext.state === 'suspended') audioContext.resume();
      const source = audioContext.createMediaStreamSource(stream);
      micAnalyser = audioContext.createAnalyser();
      micAnalyser.fftSize = 512;
      source.connect(micAnalyser);
      const buf = new Float32Array(micAnalyser.fftSize);

      if (micMonitorInterval) clearInterval(micMonitorInterval);
      micMonitorInterval = setInterval(() => {
        if (!micAnalyser) return;
        micAnalyser.getFloatTimeDomainData(buf);
        let rms = 0;
        for (let i = 0; i < buf.length; i++) rms += buf[i] * buf[i];
        rms = Math.sqrt(rms / buf.length);

        if (rms > SPEECH_THRESHOLD) {
          if (!speechDetected) {
            speechDetected = true;
            if (!currentTrace) startBrowserTrace();
            if (!currentTrace.t_user_speech_start) {
              currentTrace.t_user_speech_start = performance.now();
            }
          }
          if (speechEndTimer) { clearTimeout(speechEndTimer); speechEndTimer = null; }
        } else if (speechDetected) {
          if (!speechEndTimer) {
            speechEndTimer = setTimeout(() => {
              if (currentTrace && !currentTrace.t_user_speech_end) {
                currentTrace.t_user_speech_end = performance.now();
                currentTrace.t_user_eot_browser = performance.now();
                log(`ğŸ“ EoT: ${(currentTrace.t_user_speech_end - currentTrace.t_user_speech_start).toFixed(0)}ms speech`, 'event');
                // D12: try finalize in case playout already detected
                finalizeTrace();
              }
              speechDetected = false;
              speechEndTimer = null;
            }, SILENCE_TIMEOUT_MS);
          }
        }
      }, 30);  // 30ms polling (~33Hz), reliable in headless
    }

    // â”€â”€ D12: Agent playout energy monitor (AnalyserNode on remote track) â”€â”€
    // NOTE: uses setInterval for headless Chromium compatibility
    let agentMonitorInterval = null;
    function monitorAgentPlayout(mediaStream) {
      if (!audioContext) audioContext = new (window.AudioContext || window.webkitAudioContext)();
      if (audioContext.state === 'suspended') audioContext.resume();
      agentAudioSource = audioContext.createMediaStreamSource(mediaStream);
      agentAnalyser = audioContext.createAnalyser();
      agentAnalyser.fftSize = 512;
      agentAudioSource.connect(agentAnalyser);
      // Also connect to destination so we hear it (no-op in headless but needed for real browser)
      agentAudioSource.connect(audioContext.destination);
      const buf = new Float32Array(agentAnalyser.fftSize);

      if (agentMonitorInterval) clearInterval(agentMonitorInterval);
      agentMonitorInterval = setInterval(() => {
        if (!agentAnalyser) return;
        agentAnalyser.getFloatTimeDomainData(buf);
        let rms = 0;
        for (let i = 0; i < buf.length; i++) rms += buf[i] * buf[i];
        rms = Math.sqrt(rms / buf.length);

        if (rms > PLAYOUT_THRESHOLD && !agentPlayoutDetected) {
          agentPlayoutDetected = true;
          onAgentFirstPlayout();
        }
      }, 30);  // 30ms polling, headless-safe
    }

    // â”€â”€ D12: Called when agent playout energy first detected â”€â”€
    function onAgentFirstPlayout() {
      if (!currentTrace) return;
      const now = performance.now();
      if (!currentTrace.t_browser_first_playout) {
        currentTrace.t_browser_first_playout = now;
        currentTrace.t_first_audio_playout = now;  // compat
        log(`ğŸ”Š Agent first playout detected (energy)`, 'audio');
      }
      finalizeTrace();
    }

    // â”€â”€ D5 compat: fallback for agent audio start via track/ActiveSpeakers events â”€â”€
    function onAgentAudioStart() {
      if (!currentTrace) return;
      const now = performance.now();
      // Only use as fallback if energy detection hasn't fired yet
      if (!currentTrace.t_browser_first_playout) {
        currentTrace.t_browser_first_playout = now;
        currentTrace.t_first_audio_playout = now;
        log(`ğŸ”Š Agent audio start (fallback)`, 'audio');
      }
      finalizeTrace();
    }

    // D12: Fallback finalization when TrackSubscribed fires
    // In headless Chromium, AnalyserNode energy detection may not work,
    // so we use track arrival + delay as a playout proxy.
    function onAgentTrackArrived() {
      if (!currentTrace || currentTrace.t_browser_first_playout) return;
      // Wait 500ms to let energy detection win if it can; otherwise use fallback
      setTimeout(() => {
        if (currentTrace && !currentTrace.t_browser_first_playout) {
          currentTrace.t_browser_first_playout = performance.now();
          currentTrace.t_first_audio_playout = currentTrace.t_browser_first_playout;
          log(`ğŸ”Š Agent playout (track-arrival fallback)`, 'audio');
          finalizeTrace();
        }
      }, 500);
    }

    function finalizeTrace() {
      if (!currentTrace) return;
      if (!currentTrace.t_browser_first_playout) return;
      if (!currentTrace.t_user_speech_end) return;

      const raw_delta = currentTrace.t_browser_first_playout - currentTrace.t_user_speech_end;
      // If agent responded before user finished (streaming), KPI = 0
      const user_kpi_ms = Math.max(0, Math.round(raw_delta));
      const raw_kpi_ms = Math.round(raw_delta);
      log(`âš¡ USER_KPI (EoTâ†’Playout): ${user_kpi_ms}ms (raw=${raw_kpi_ms}ms)`, 'success');

      const traceData = {
        ...currentTrace,
        user_kpi_ms,
        user_kpi_raw_ms: raw_kpi_ms,
        eot_to_first_audio_ms: user_kpi_ms,
        wall_time: Date.now(),
      };
      browserTraces.push(traceData);
      window.__autobrowser_traces = browserTraces;
      updateStats();

      // Reset for next turn
      currentTrace = null;
      startBrowserTrace();
    }

    function updateStats() {
      const el = document.getElementById('stats');
      if (!el || browserTraces.length === 0) return;
      const vals = browserTraces.map(t => t.user_kpi_ms).filter(v => v > 0).sort((a,b) => a-b);
      if (vals.length === 0) return;
      const p50 = vals[Math.floor(vals.length * 0.5)];
      const p95 = vals[Math.floor(vals.length * 0.95)];
      el.textContent = `ğŸ“Š USER_KPI: P50=${p50}ms P95=${p95}ms (n=${vals.length})`;
    }

    // â”€â”€ D12: Audio recording (record agent's remote audio stream) â”€â”€
    function startRecording(mediaStream) {
      recordedChunks = [];
      try {
        // Try opus first, then fallback
        const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus')
          ? 'audio/webm;codecs=opus'
          : 'audio/webm';
        mediaRecorder = new MediaRecorder(mediaStream, { mimeType });
        mediaRecorder.ondataavailable = (e) => {
          if (e.data.size > 0) recordedChunks.push(e.data);
        };
        mediaRecorder.onstop = () => {
          const blob = new Blob(recordedChunks, { type: mediaRecorder.mimeType });
          window.__autobrowser_recording_blob = blob;
          log(`ğŸ™ï¸ Recording stopped: ${(blob.size / 1024).toFixed(1)}KB`, 'audio');
        };
        mediaRecorder.start(1000); // chunk every 1s
        log(`ğŸ™ï¸ Recording agent audio (${mimeType})`, 'audio');
      } catch(e) {
        log(`Recording start failed: ${e.message}`, 'error');
      }
    }

    function stopRecording() {
      if (mediaRecorder && mediaRecorder.state !== 'inactive') {
        mediaRecorder.stop();
      }
    }

    // â”€â”€ Logging / UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    function log(msg, type = 'info') {
      const logEl = document.getElementById('log');
      if (!logEl) return;
      const entry = document.createElement('div');
      entry.className = `log-entry ${type}`;
      entry.textContent = `[${new Date().toLocaleTimeString()}] ${msg}`;
      logEl.appendChild(entry);
      logEl.scrollTop = logEl.scrollHeight;
      // Also console.log for Playwright capture
      console.log(`[autobrowser:${type}] ${msg}`);
    }

    function setStatus(text, cls) {
      const el = document.getElementById('status');
      el.textContent = text;
      el.className = `status ${cls}`;
    }

    function setVisualizer(state) {
      const el = document.getElementById('visualizer');
      el.className = 'visualizer' + (state ? ' ' + state : '');
      const textMap = {
        '': 'ğŸ¤ ç­‰å¾…è¿æ¥',
        'active': 'ğŸ¤ æ­£åœ¨è†å¬...',
        'speaking': 'ğŸ”Š æ­£åœ¨å›å¤...',
      };
      el.querySelector('.visualizer-text').textContent = textMap[state || ''] || 'ğŸ¤';
    }

    async function unlockPlayback() {
      try {
        const Ctx = window.AudioContext || window.webkitAudioContext;
        if (Ctx) {
          if (!audioContext) audioContext = new Ctx();
          if (audioContext.state === 'suspended') {
            await audioContext.resume();
          }
        }
      } catch (e) {
        log(`AudioContext resume failed: ${e.message || e}`, 'error');
      }
      try {
        if (agentAudioEl) {
          agentAudioEl.muted = false;
          agentAudioEl.volume = 1.0;
          await agentAudioEl.play();
        }
      } catch (e) {
        log(`Audio unlock play failed: ${e.message || e}`, 'error');
      }
    }

    // â”€â”€ å†…åµŒ Token (7å¤©æœ‰æ•ˆ, åˆ°æœŸååœ¨æœåŠ¡å™¨ä¸Šé‡æ–°ç”Ÿæˆ) â”€â”€
    const EMBEDDED_TOKEN = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJuYW1lIjoiVXNlciIsInZpZGVvIjp7InJvb21Kb2luIjp0cnVlLCJyb29tIjoidm9pY2UtYWdlbnQtdGVzdCIsImNhblB1Ymxpc2giOnRydWUsImNhblN1YnNjcmliZSI6dHJ1ZSwiY2FuUHVibGlzaERhdGEiOnRydWV9LCJzdWIiOiJ2b2ljZS11c2VyLTEiLCJpc3MiOiJBUEk3ZmozNXdHTHVtdGMiLCJuYmYiOjE3NzA5NDc3MzksImV4cCI6MTc3MTU1MjUzOX0.RhGZpVOEAEiVa4mP6A4-WQD4e6ybjK6w75oB-KePDG8';
    const LIVEKIT_WS = 'wss://renshenghehuoren-mpdsjfwe.livekit.cloud';

    async function getToken() {
      // D12: Auto mode â€” use URL params directly
      if (PARAM_TOKEN && PARAM_LK_URL) {
        log(`ä½¿ç”¨ URL å‚æ•° Token (room=${PARAM_ROOM})`, 'info');
        return { token: PARAM_TOKEN, url: PARAM_LK_URL };
      }

      // 1. URL å‚æ•° (legacy compat)
      if (_params.get('lk_token')) {
        log('ä½¿ç”¨ URL å‚æ•°ä¸­çš„ Token', 'info');
        return { token: _params.get('lk_token'), url: LIVEKIT_WS };
      }

      // 2. å°è¯• Token APIï¼ˆå¦‚æœ Token Server å¯è¾¾ï¼‰
      const tokenParams = new URLSearchParams();
      if (PARAM_ROOM) tokenParams.set('room', PARAM_ROOM);
      if (PARAM_IDENTITY) tokenParams.set('identity', PARAM_IDENTITY);
      const qs = tokenParams.toString() ? `?${tokenParams.toString()}` : '';

      for (const base of [window.location.origin, '']) {
        try {
          const resp = await fetch(`${base}/api/token${qs}`);
          if (resp.ok) {
            const data = await resp.json();
            log(`Token API: room=${data.room}, TTL=${data.ttl}s`, 'success');
            return { token: data.token, url: data.url };
          }
        } catch (e) { /* try next */ }
      }

      // 3. å†…åµŒ Tokenï¼ˆæœ€ç»ˆ fallbackï¼Œ7å¤©æœ‰æ•ˆï¼‰
      log('ä½¿ç”¨å†…åµŒ Token (7å¤©æœ‰æ•ˆ)', 'info');
      return { token: EMBEDDED_TOKEN, url: LIVEKIT_WS };
    }

    async function connect() {
      try {
        const btn = document.getElementById('connectBtn');
        btn.disabled = true;
        setStatus('æ­£åœ¨è¿æ¥...', 'connecting');
        await unlockPlayback();

        const { token, url } = await getToken();
        log(`è¿æ¥åˆ° ${url}`, 'event');

        room = new LivekitClient.Room({
          adaptiveStream: true,
          dynacast: true,
          audioCaptureDefaults: {
            autoGainControl: true,
            echoCancellation: true,
            noiseSuppression: true,
          },
        });
        window.room = room;  // D12: expose for Playwright

        // äº‹ä»¶ç›‘å¬
        room.on(LivekitClient.RoomEvent.Connected, () => {
          log('âœ… å·²è¿æ¥åˆ°æˆ¿é—´', 'success');
          setStatus('å·²è¿æ¥', 'connected');
          setVisualizer('active');
          document.getElementById('disconnectBtn').disabled = false;
          window.__autobrowser_joined = true;
        });

        room.on(LivekitClient.RoomEvent.Disconnected, (reason) => {
          log(`æ–­å¼€è¿æ¥: ${reason || 'unknown'}`, 'event');
          setStatus('å·²æ–­å¼€', 'disconnected');
          setVisualizer('');
          btn.disabled = false;
          document.getElementById('disconnectBtn').disabled = true;
          stopRecording();
        });

        room.on(LivekitClient.RoomEvent.ParticipantConnected, (p) => {
          log(`å‚ä¸è€…åŠ å…¥: ${p.identity}`, 'event');
        });

        room.on(LivekitClient.RoomEvent.TrackSubscribed, (track, pub, participant) => {
          log(`æ”¶åˆ° ${participant.identity} çš„ ${track.kind} track`, 'audio');
          if (track.kind === 'audio') {
            // D12: Record t_agent_track_first_frame_recv
            if (currentTrace && !currentTrace.t_agent_track_first_frame_recv) {
              currentTrace.t_agent_track_first_frame_recv = performance.now();
              log(`ğŸ“ Agent track first frame recv: ${currentTrace.t_agent_track_first_frame_recv.toFixed(0)}ms`, 'event');
            }

            if (agentAudioEl) { agentAudioEl.remove(); }
            agentAudioEl = track.attach();
            agentAudioEl.style.display = 'none';
            agentAudioEl.autoplay = true;
            agentAudioEl.playsInline = true;
            agentAudioEl.muted = false;
            agentAudioEl.volume = 1.0;
            document.body.appendChild(agentAudioEl);
            unlockPlayback().catch(e => log(`Audio play error: ${e}`, 'error'));
            log('ğŸ”Š Agent éŸ³é¢‘å·²è¿æ¥', 'success');
            setVisualizer('speaking');

            // D12: Start energy-based playout detection on remote track
            try {
              const remoteStream = new MediaStream([track.mediaStreamTrack]);
              monitorAgentPlayout(remoteStream);
              // D12: Start recording agent audio
              startRecording(remoteStream);
              // D12: Fallback finalization if EoT already detected
              onAgentTrackArrived();
            } catch(e) {
              log(`Agent playout monitor failed: ${e.message}`, 'error');
              // Fallback to old method
              onAgentAudioStart();
            }
          }
        });

        room.on(LivekitClient.RoomEvent.TrackUnsubscribed, (track) => {
          if (track.kind === 'audio') {
            track.detach().forEach(el => el.remove());
            setVisualizer('active');
          }
        });

        // è¿æ¥
        await room.connect(url, token);
        log('æ­£åœ¨å‘å¸ƒéº¦å…‹é£...', 'info');
        await room.localParticipant.setMicrophoneEnabled(true);
        log('ğŸ¤ éº¦å…‹é£å·²å¼€å¯', 'success');

        // ç›‘æ§éº¦å…‹é£èƒ½é‡ï¼ˆæµè§ˆå™¨ç«¯ EoT æ£€æµ‹ï¼‰
        try {
          const micTrack = room.localParticipant.getTrackPublication(LivekitClient.Track.Source.Microphone);
          if (micTrack && micTrack.track) {
            const stream = new MediaStream([micTrack.track.mediaStreamTrack]);
            monitorMic(stream);
            log('ğŸ“Š æµè§ˆå™¨ç«¯å»¶è¿Ÿç›‘æ§å·²å¯åŠ¨', 'info');
          }
        } catch(e) { log(`Mic monitor: ${e.message}`, 'info'); }

        // ç›‘å¬ ActiveSpeakers ä½œä¸ºé¦–éŸ³ fallback
        room.on(LivekitClient.RoomEvent.ActiveSpeakersChanged, (speakers) => {
          const localIdentity = room.localParticipant.identity;
          const agentSpeaking = speakers.some(s => s.identity !== localIdentity);
          setVisualizer(agentSpeaking ? 'speaking' : 'active');
          if (agentSpeaking) onAgentAudioStart();
        });

        startBrowserTrace();

        // D12: Auto-disconnect timer for automation
        if (AUTO_MODE && AUTO_DISCONNECT_S > 0) {
          log(`â±ï¸ Auto mode: will disconnect in ${AUTO_DISCONNECT_S}s`, 'info');
          setTimeout(async () => {
            log('â±ï¸ Auto disconnect triggered', 'event');
            stopRecording();
            // Allow recording to finalize
            await new Promise(r => setTimeout(r, 500));
            // Send callback
            await sendCallback();
            window.__autobrowser_done = true;
            await disconnect();
          }, AUTO_DISCONNECT_S * 1000);
        }

      } catch (e) {
        log(`è¿æ¥å¤±è´¥: ${e.message}`, 'error');
        setStatus('è¿æ¥å¤±è´¥', 'error');
        document.getElementById('connectBtn').disabled = false;
        window.__autobrowser_error = e.message;
        window.__autobrowser_done = true;
      }
    }

    async function disconnect() {
      stopRecording();
      if (room) {
        await room.disconnect();
        room = null;
        window.room = null;
      }
      if (agentAudioEl) {
        agentAudioEl.remove();
        agentAudioEl = null;
      }
      // Clean up audio nodes
      if (agentAnalyser) { agentAnalyser = null; }
      if (agentAudioSource) { agentAudioSource = null; }
      setStatus('å·²æ–­å¼€', 'disconnected');
      setVisualizer('');
      document.getElementById('connectBtn').disabled = false;
      document.getElementById('disconnectBtn').disabled = true;
      log('å·²æ–­å¼€è¿æ¥', 'event');
    }

    // â”€â”€ D12: HTTP callback to send browser_trace back to harness â”€â”€
    async function sendCallback() {
      if (!PARAM_CALLBACK) return;
      const payload = {
        trace_id: PARAM_TRACE_ID,
        case_id: PARAM_CASE_ID,
        turn_id: PARAM_TURN_ID,
        traces: browserTraces,
        has_recording: !!window.__autobrowser_recording_blob,
        recording_size: window.__autobrowser_recording_blob?.size || 0,
      };
      try {
        await fetch(PARAM_CALLBACK, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify(payload),
        });
        log(`ğŸ“¤ Callback sent to ${PARAM_CALLBACK}`, 'success');
      } catch(e) {
        log(`Callback failed: ${e.message}`, 'error');
      }
    }

    // â”€â”€ D12: Auto-connect on page load if auto=1 â”€â”€
    if (AUTO_MODE) {
      window.addEventListener('load', () => {
        log(`ğŸ¤– AutoBrowser mode: trace=${PARAM_TRACE_ID} case=${PARAM_CASE_ID}`, 'event');
        setTimeout(() => connect(), 500);
      });
    }
  </script>
</body>
</html>
