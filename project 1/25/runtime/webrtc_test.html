<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Qwen3 ËØ≠Èü≥Âä©Êâã</title>
  <script src="https://cdn.jsdelivr.net/npm/livekit-client@2.9.1/dist/livekit-client.umd.js"></script>
  <style>
    * { margin: 0; padding: 0; box-sizing: border-box; }
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      background: linear-gradient(135deg, #0f0f23 0%, #1a1a3e 50%, #0f0f23 100%);
      color: #e0e0e0;
      min-height: 100vh;
      display: flex;
      flex-direction: column;
      align-items: center;
      padding: 20px;
    }
    h1 {
      font-size: 1.6em;
      margin: 20px 0 10px;
      background: linear-gradient(90deg, #6366f1, #8b5cf6);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
    }
    .subtitle { color: #888; font-size: 0.85em; margin-bottom: 20px; }
    .status {
      padding: 8px 20px;
      border-radius: 20px;
      font-size: 0.9em;
      font-weight: 500;
      margin-bottom: 15px;
      transition: all 0.3s;
    }
    .status.disconnected { background: rgba(239,68,68,0.15); color: #ef4444; border: 1px solid rgba(239,68,68,0.3); }
    .status.connecting { background: rgba(234,179,8,0.15); color: #eab308; border: 1px solid rgba(234,179,8,0.3); }
    .status.connected { background: rgba(34,197,94,0.15); color: #22c55e; border: 1px solid rgba(34,197,94,0.3); }
    .status.error { background: rgba(239,68,68,0.15); color: #ef4444; border: 1px solid rgba(239,68,68,0.3); }

    .controls {
      display: flex;
      gap: 10px;
      margin-bottom: 20px;
    }
    button {
      padding: 10px 28px;
      border: none;
      border-radius: 10px;
      font-size: 1em;
      cursor: pointer;
      transition: all 0.2s;
      font-weight: 500;
    }
    #connectBtn {
      background: linear-gradient(135deg, #6366f1, #8b5cf6);
      color: white;
      box-shadow: 0 4px 15px rgba(99,102,241,0.3);
    }
    #connectBtn:hover:not(:disabled) { transform: translateY(-1px); box-shadow: 0 6px 20px rgba(99,102,241,0.4); }
    #connectBtn:disabled { opacity: 0.5; cursor: not-allowed; }
    #disconnectBtn {
      background: rgba(239,68,68,0.15);
      color: #ef4444;
      border: 1px solid rgba(239,68,68,0.3);
    }
    #disconnectBtn:hover:not(:disabled) { background: rgba(239,68,68,0.25); }
    #disconnectBtn:disabled { opacity: 0.3; cursor: not-allowed; }

    .visualizer {
      width: 200px;
      height: 200px;
      border-radius: 50%;
      background: radial-gradient(circle, rgba(99,102,241,0.1) 0%, transparent 70%);
      border: 2px solid rgba(99,102,241,0.2);
      display: flex;
      align-items: center;
      justify-content: center;
      margin: 15px 0;
      transition: all 0.3s;
    }
    .visualizer.active {
      border-color: rgba(99,102,241,0.6);
      box-shadow: 0 0 40px rgba(99,102,241,0.2);
      animation: pulse 2s ease-in-out infinite;
    }
    .visualizer.speaking {
      border-color: rgba(34,197,94,0.6);
      box-shadow: 0 0 40px rgba(34,197,94,0.2);
      animation: pulse-green 1s ease-in-out infinite;
    }
    @keyframes pulse {
      0%, 100% { transform: scale(1); }
      50% { transform: scale(1.03); }
    }
    @keyframes pulse-green {
      0%, 100% { transform: scale(1); }
      50% { transform: scale(1.05); }
    }
    .visualizer-text {
      font-size: 1.1em;
      color: #888;
    }

    .log-container {
      width: 100%;
      max-width: 600px;
      margin-top: 15px;
    }
    .log-header {
      display: flex;
      justify-content: space-between;
      align-items: center;
      margin-bottom: 8px;
    }
    .log-header h3 { font-size: 0.9em; color: #888; }
    .log-header button {
      padding: 4px 12px;
      font-size: 0.75em;
      background: rgba(255,255,255,0.05);
      color: #888;
      border: 1px solid rgba(255,255,255,0.1);
      border-radius: 6px;
    }
    #log {
      background: rgba(0,0,0,0.3);
      border: 1px solid rgba(255,255,255,0.06);
      border-radius: 12px;
      padding: 12px;
      height: 200px;
      overflow-y: auto;
      font-family: 'SF Mono', Monaco, monospace;
      font-size: 0.8em;
      line-height: 1.6;
    }
    .log-entry.info { color: #94a3b8; }
    .log-entry.event { color: #6366f1; }
    .log-entry.success { color: #22c55e; }
    .log-entry.error { color: #ef4444; }
    .log-entry.audio { color: #eab308; }
  </style>
</head>
<body>
  <h1>üéôÔ∏è Qwen3 ËØ≠Èü≥Âä©Êâã</h1>
  <p class="subtitle">WebRTC ÂÆûÊó∂ËØ≠Èü≥ÈÄöËØù ¬∑ Omni + TTS</p>

  <div id="status" class="status disconnected">Êú™ËøûÊé•</div>
  <div id="stats" style="color:#6366f1; font-size:0.85em; margin:5px 0; font-family:monospace;"></div>

  <div class="controls">
    <button id="connectBtn" onclick="connect()">üîå ËøûÊé•</button>
    <button id="disconnectBtn" onclick="disconnect()" disabled>Êñ≠ÂºÄ</button>
  </div>

  <div id="visualizer" class="visualizer">
    <span class="visualizer-text">üé§ Á≠âÂæÖËøûÊé•</span>
  </div>

  <div class="log-container">
    <div class="log-header">
      <h3>Êó•Âøó</h3>
      <button onclick="document.getElementById('log').innerHTML=''">Ê∏ÖÁ©∫</button>
    </div>
    <div id="log"></div>
  </div>

  <script>
    let room = null;
    let agentAudioEl = null;

    // ‚îÄ‚îÄ D12: URL params for auto mode ‚îÄ‚îÄ
    const _params = new URLSearchParams(window.location.search);
    const AUTO_MODE = _params.get('auto') === '1';
    const PARAM_TOKEN = _params.get('lk_token') || '';
    const PARAM_LK_URL = _params.get('lk_url') || '';
    const PARAM_ROOM = _params.get('room') || '';
    const PARAM_IDENTITY = _params.get('identity') || '';
    const PARAM_TRACE_ID = _params.get('trace_id') || '';
    const PARAM_CASE_ID = _params.get('case_id') || '';
    const PARAM_TURN_ID = _params.get('turn_id') || '0';
    const AUTO_DISCONNECT_S = parseInt(_params.get('auto_disconnect_s') || '0', 10);

    // ‚îÄ‚îÄ D12: Globals exposed to Playwright ‚îÄ‚îÄ
    window.__autobrowser_joined = false;
    window.__autobrowser_done = false;
    window.__autobrowser_error = null;
    window.__autobrowser_traces = [];
    window.__autobrowser_recording_blob = null;

    // ‚îÄ‚îÄ D13: Browser-side instrumentation ‚îÄ‚îÄ
    const browserTraces = [];
    let currentTrace = null;
    let audioContext = null;
    let micAnalyser = null;
    let agentAnalyser = null;
    let agentPlayoutDetected = false;
    let speechDetected = false;
    let speechEndTimer = null;
    let micMonitorInterval = null;
    let agentMonitorInterval = null;
    let mediaRecorder = null;
    let recordedChunks = [];
    const SPEECH_THRESHOLD = 0.015;
    const SILENCE_TIMEOUT_MS = parseInt(_params.get('silence_timeout_ms') || (AUTO_MODE ? '1500' : '400'), 10);

    // D13 P0-3: Playout detection resolution (ms)
    const PLAYOUT_POLL_MS = parseInt(_params.get('playout_poll_ms') || '5', 10);
    const MIC_POLL_MS = parseInt(_params.get('mic_poll_ms') || '10', 10);

    // D14 P0-2: Ground-truth speech end time from offline WAV analysis (ms from WAV start)
    const GT_SPEECH_END_MS = parseFloat(_params.get('gt_speech_end_ms') || '0');
    const GT_TALK_OVER_MARGIN_MS = 50;

    function startBrowserTrace() {
      currentTrace = {
        trace_id: PARAM_TRACE_ID,
        case_id: PARAM_CASE_ID,
        turn_id: PARAM_TURN_ID,
        t_user_speech_start: null,
        t_user_speech_end: null,
        t_user_eot_browser: null,
        t_agent_track_first_frame_recv: null,
        t_browser_first_playout: null,
        t_first_audio_playout: null,
      };
      speechDetected = false;
    }

    // D13 P0-1: finalizeTrace ‚Äî ONLY finalize when we have BOTH EoT and playout.
    // force=true is used when a new speech turn starts to flush the old incomplete trace.
    let _finalize_timeout = null;
    function finalizeTrace(force) {
      if (!currentTrace) return;
      const hasPlayout = currentTrace.t_browser_first_playout != null;
      const hasEot = currentTrace.t_user_speech_end != null;

      // Normal path: wait until we have BOTH timestamps
      if (!force && (!hasPlayout || !hasEot)) {
        // Schedule a timeout: if after EoT we don't get playout in 8s, force-finalize
        if (hasEot && !_finalize_timeout) {
          _finalize_timeout = setTimeout(() => {
            _finalize_timeout = null;
            log('Finalize timeout (no playout after EoT)', 'event');
            finalizeTrace(true);
          }, 8000);
        }
        return; // Don't finalize yet ‚Äî wait for the missing piece
      }

      if (_finalize_timeout) { clearTimeout(_finalize_timeout); _finalize_timeout = null; }

      let raw_kpi_ms = null;
      let user_kpi_clamped = null;
      let is_talk_over = false;

      if (hasPlayout && hasEot) {
        const raw_delta = currentTrace.t_browser_first_playout - currentTrace.t_user_speech_end;
        raw_kpi_ms = Math.round(raw_delta);
        user_kpi_clamped = Math.max(0, raw_kpi_ms);
        is_talk_over = raw_kpi_ms < 0;
      }

      const label = (hasPlayout && hasEot)
        ? (is_talk_over ? `TALK-OVER raw=${raw_kpi_ms}ms` : `USER_KPI raw=${raw_kpi_ms}ms clamped=${user_kpi_clamped}ms`)
        : `Trace incomplete (eot=${hasEot} playout=${hasPlayout})`;
      log(label, (hasPlayout && hasEot) ? (is_talk_over ? 'event' : 'success') : 'info');

      // D14 P0-2: Ground-truth EoT calibration
      let t_user_eot_gt = null;
      let browser_eot_lag_ms = null;
      let is_talk_over_gt = null;
      if (GT_SPEECH_END_MS > 0 && currentTrace.t_user_speech_start != null) {
        // GT speech end time relative to performance.now() timeline
        t_user_eot_gt = currentTrace.t_user_speech_start + GT_SPEECH_END_MS;
        if (hasEot) {
          browser_eot_lag_ms = Math.round(currentTrace.t_user_speech_end - t_user_eot_gt);
        }
        if (hasPlayout) {
          is_talk_over_gt = currentTrace.t_browser_first_playout < (t_user_eot_gt - GT_TALK_OVER_MARGIN_MS);
        }
      }

      const traceData = {
        ...currentTrace,
        user_kpi_raw_ms: raw_kpi_ms,
        user_kpi_ms: user_kpi_clamped,
        is_talk_over,
        eot_to_first_audio_ms: user_kpi_clamped,
        playout_resolution_ms: PLAYOUT_POLL_MS,
        mic_resolution_ms: MIC_POLL_MS,
        gt_speech_end_ms: GT_SPEECH_END_MS > 0 ? GT_SPEECH_END_MS : null,
        t_user_eot_gt,
        browser_eot_lag_ms,
        is_talk_over_gt,
        wall_time: Date.now(),
      };
      browserTraces.push(traceData);
      window.__autobrowser_traces = browserTraces;
      updateStats();

      currentTrace = null;
      startBrowserTrace();
    }

    // D13 P0-3: Mic monitoring at configurable resolution
    function monitorMic(stream) {
      const Ctx = window.AudioContext || window.webkitAudioContext;
      if (!audioContext && Ctx) audioContext = new Ctx();
      const source = audioContext.createMediaStreamSource(stream);
      micAnalyser = audioContext.createAnalyser();
      micAnalyser.fftSize = 512;
      source.connect(micAnalyser);
      const buf = new Float32Array(micAnalyser.fftSize);

      micMonitorInterval = setInterval(() => {
        if (!micAnalyser) return;
        micAnalyser.getFloatTimeDomainData(buf);
        let rms = 0;
        for (let i = 0; i < buf.length; i++) rms += buf[i] * buf[i];
        rms = Math.sqrt(rms / buf.length);

        if (rms > SPEECH_THRESHOLD) {
          if (!speechDetected) {
            speechDetected = true;
            // New speech detected ‚Äî if previous trace has data, flush it
            if (currentTrace && (currentTrace.t_user_speech_end != null || currentTrace.t_browser_first_playout != null)) {
              finalizeTrace(true);
            }
            if (!currentTrace) startBrowserTrace();
            if (!currentTrace.t_user_speech_start) {
              currentTrace.t_user_speech_start = performance.now();
            }
            // Reset playout detection for new turn
            agentPlayoutDetected = false;
          }
          if (speechEndTimer) { clearTimeout(speechEndTimer); speechEndTimer = null; }
        } else if (speechDetected) {
          if (!speechEndTimer) {
            speechEndTimer = setTimeout(() => {
              if (currentTrace && !currentTrace.t_user_speech_end) {
                currentTrace.t_user_speech_end = performance.now();
                currentTrace.t_user_eot_browser = currentTrace.t_user_speech_end;
                log(`EoT: ${(currentTrace.t_user_speech_end - currentTrace.t_user_speech_start).toFixed(0)}ms speech`, 'event');
                // D13: try finalize ‚Äî will succeed if playout already recorded
                finalizeTrace();
              }
              speechDetected = false;
              speechEndTimer = null;
            }, SILENCE_TIMEOUT_MS);
          }
        }
      }, MIC_POLL_MS);
    }

    // D13 P0-3: Agent playout detection at high resolution
    function monitorAgentPlayout(audioEl) {
      if (!audioEl || !audioEl.srcObject) return;
      try {
        const Ctx = window.AudioContext || window.webkitAudioContext;
        if (!audioContext && Ctx) audioContext = new Ctx();
        const source = audioContext.createMediaStreamSource(audioEl.srcObject);
        agentAnalyser = audioContext.createAnalyser();
        agentAnalyser.fftSize = 256;  // smaller FFT for faster processing
        source.connect(agentAnalyser);
        const buf = new Float32Array(agentAnalyser.fftSize);
        const PLAYOUT_THRESHOLD = 0.005;

        agentMonitorInterval = setInterval(() => {
          if (!agentAnalyser || agentPlayoutDetected) return;
          agentAnalyser.getFloatTimeDomainData(buf);
          let rms = 0;
          for (let i = 0; i < buf.length; i++) rms += buf[i] * buf[i];
          rms = Math.sqrt(rms / buf.length);

          if (rms > PLAYOUT_THRESHOLD) {
            agentPlayoutDetected = true;
            if (currentTrace && !currentTrace.t_browser_first_playout) {
              currentTrace.t_browser_first_playout = performance.now();
              currentTrace.t_first_audio_playout = currentTrace.t_browser_first_playout;
              log(`Agent playout detected (energy=${rms.toFixed(4)}, res=${PLAYOUT_POLL_MS}ms)`, 'success');
              // D13: try finalize ‚Äî will only succeed if EoT already recorded
              finalizeTrace();
            }
          }
        }, PLAYOUT_POLL_MS);

        startRecording(audioEl.srcObject);
      } catch (e) {
        console.log('[autobrowser] monitorAgentPlayout error:', e);
      }
    }

    function startRecording(stream) {
      try {
        recordedChunks = [];
        mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm;codecs=opus' });
        mediaRecorder.ondataavailable = (e) => {
          if (e.data && e.data.size > 0) recordedChunks.push(e.data);
        };
        mediaRecorder.onstop = () => {
          if (recordedChunks.length > 0) {
            window.__autobrowser_recording_blob = new Blob(recordedChunks, { type: 'audio/webm' });
          }
        };
        mediaRecorder.start(500);
      } catch (e) {
        console.log('[autobrowser] MediaRecorder error:', e);
      }
    }

    function stopRecording() {
      if (mediaRecorder && mediaRecorder.state !== 'inactive') {
        try { mediaRecorder.stop(); } catch(e) {}
      }
    }

    function onAgentAudioStart() {
      if (currentTrace && !currentTrace.t_first_audio_playout) {
        currentTrace.t_first_audio_playout = performance.now();
        currentTrace.t_browser_first_playout = currentTrace.t_first_audio_playout;
        log(`Agent audio start (ActiveSpeakers/TrackSubscribed)`, 'success');
        finalizeTrace();
      }
    }

    // D12 legacy fallback ‚Äî kept but NOT called by default in D13.
    // D13 uses padded WAV so monitorMic naturally detects EoT.
    window.resetForMeasurement = function() {
      browserTraces.length = 0;
      window.__autobrowser_traces = [];
      agentPlayoutDetected = false;
      startBrowserTrace();
      const eotNow = performance.now();
      currentTrace.t_user_speech_start = eotNow - 100;
      currentTrace.t_user_speech_end = eotNow;
      currentTrace.t_user_eot_browser = eotNow;
      speechDetected = false;
      console.log('[autobrowser:event] Trace reset (legacy fallback)');
    };

    function updateStats() {
      const el = document.getElementById('stats');
      if (!el || browserTraces.length === 0) return;
      const rawVals = browserTraces.map(t => t.user_kpi_raw_ms).filter(v => v != null).sort((a,b) => a-b);
      if (rawVals.length === 0) return;
      const talkOvers = rawVals.filter(v => v < 0);
      const turnTaking = rawVals.filter(v => v >= 0).sort((a,b) => a-b);
      const p50 = turnTaking.length > 0 ? turnTaking[Math.floor(turnTaking.length * 0.5)] : 'N/A';
      const p95 = turnTaking.length > 0 ? turnTaking[Math.floor(turnTaking.length * 0.95)] : 'N/A';
      let txt = `KPI raw: P50=${p50}ms P95=${p95}ms (n=${rawVals.length})`;
      if (talkOvers.length > 0) txt += ` | talk-over: ${talkOvers.length}`;
      el.textContent = txt;
    }

    function log(msg, type = 'info') {
      const logEl = document.getElementById('log');
      if (!logEl) { console.log(`[log:${type}] ${msg}`); return; }
      const entry = document.createElement('div');
      entry.className = `log-entry ${type}`;
      entry.textContent = `[${new Date().toLocaleTimeString()}] ${msg}`;
      logEl.appendChild(entry);
      logEl.scrollTop = logEl.scrollHeight;
      console.log(`[autobrowser:${type}] ${msg}`);
    }

    function setStatus(text, cls) {
      const el = document.getElementById('status');
      if (el) { el.textContent = text; el.className = `status ${cls}`; }
    }

    function setVisualizer(state) {
      const el = document.getElementById('visualizer');
      if (!el) return;
      el.className = 'visualizer' + (state ? ' ' + state : '');
      const textMap = {
        '': 'üé§ Á≠âÂæÖËøûÊé•',
        'active': 'üé§ Ê≠£Âú®ËÅÜÂê¨...',
        'speaking': 'üîä Ê≠£Âú®ÂõûÂ§ç...',
      };
      const span = el.querySelector('.visualizer-text');
      if (span) span.textContent = textMap[state || ''] || 'üé§';
    }

    async function unlockPlayback() {
      try {
        const Ctx = window.AudioContext || window.webkitAudioContext;
        if (Ctx) {
          if (!audioContext) audioContext = new Ctx();
          if (audioContext.state === 'suspended') await audioContext.resume();
        }
      } catch (e) { log(`AudioContext resume: ${e}`, 'error'); }
      try {
        if (agentAudioEl) { agentAudioEl.muted = false; agentAudioEl.volume = 1.0; await agentAudioEl.play(); }
      } catch (e) { log(`Audio unlock: ${e}`, 'error'); }
    }

    const EMBEDDED_TOKEN = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJuYW1lIjoiVXNlciIsInZpZGVvIjp7InJvb21Kb2luIjp0cnVlLCJyb29tIjoidm9pY2UtYWdlbnQtdGVzdCIsImNhblB1Ymxpc2giOnRydWUsImNhblN1YnNjcmliZSI6dHJ1ZSwiY2FuUHVibGlzaERhdGEiOnRydWV9LCJzdWIiOiJ2b2ljZS11c2VyLTEiLCJpc3MiOiJBUEk3ZmozNXdHTHVtdGMiLCJuYmYiOjE3NzA5NDc3MzksImV4cCI6MTc3MTU1MjUzOX0.RhGZpVOEAEiVa4mP6A4-WQD4e6ybjK6w75oB-KePDG8';
    const LIVEKIT_WS = 'wss://renshenghehuoren-mpdsjfwe.livekit.cloud';

    async function getToken() {
      const params = new URLSearchParams(window.location.search);
      if (params.get('lk_token')) {
        const url = params.get('lk_url') || LIVEKIT_WS;
        log(`Using URL token, url=${url}`, 'info');
        return { token: params.get('lk_token'), url };
      }
      for (const base of [window.location.origin, '']) {
        try {
          const resp = await fetch(`${base}/api/token`);
          if (resp.ok) {
            const data = await resp.json();
            log(`Token API: room=${data.room}`, 'success');
            return { token: data.token, url: data.url };
          }
        } catch (e) {}
      }
      log('Using embedded token', 'info');
      return { token: EMBEDDED_TOKEN, url: LIVEKIT_WS };
    }

    async function connect() {
      try {
        const btn = document.getElementById('connectBtn');
        if (btn) btn.disabled = true;
        setStatus('Connecting...', 'connecting');
        await unlockPlayback();

        const { token, url } = await getToken();
        log(`Connecting to ${url}`, 'event');

        room = new LivekitClient.Room({
          adaptiveStream: true,
          dynacast: true,
          audioCaptureDefaults: { autoGainControl: true, echoCancellation: true, noiseSuppression: true },
        });
        window.room = room;

        room.on(LivekitClient.RoomEvent.Connected, () => {
          log('Connected to room', 'success');
          setStatus('Connected', 'connected');
          setVisualizer('active');
          const db = document.getElementById('disconnectBtn');
          if (db) db.disabled = false;
          window.__autobrowser_joined = true;
        });

        room.on(LivekitClient.RoomEvent.Disconnected, (reason) => {
          log(`Disconnected: ${reason || 'unknown'}`, 'event');
          setStatus('Disconnected', 'disconnected');
          setVisualizer('');
          if (btn) btn.disabled = false;
          const db = document.getElementById('disconnectBtn');
          if (db) db.disabled = true;
          stopRecording();
          window.__autobrowser_done = true;
        });

        room.on(LivekitClient.RoomEvent.ParticipantConnected, (p) => {
          log(`Participant joined: ${p.identity}`, 'event');
        });

        room.on(LivekitClient.RoomEvent.TrackSubscribed, (track, pub, participant) => {
          log(`Track subscribed: ${participant.identity} ${track.kind}`, 'audio');
          if (track.kind === 'audio') {
            if (agentAudioEl) { agentAudioEl.remove(); }
            agentAudioEl = track.attach();
            agentAudioEl.style.display = 'none';
            agentAudioEl.autoplay = true;
            agentAudioEl.playsInline = true;
            agentAudioEl.muted = false;
            agentAudioEl.volume = 1.0;
            document.body.appendChild(agentAudioEl);
            unlockPlayback().catch(() => {});
            log('Agent audio attached', 'success');
            setVisualizer('speaking');

            // Record timestamp for first frame
            if (currentTrace && !currentTrace.t_agent_track_first_frame_recv) {
              currentTrace.t_agent_track_first_frame_recv = performance.now();
            }

            // Start agent playout monitoring
            monitorAgentPlayout(agentAudioEl);

            // Fallback: if playout not detected in 500ms, use track subscribe time
            setTimeout(() => {
              if (currentTrace && !currentTrace.t_browser_first_playout) {
                currentTrace.t_browser_first_playout = performance.now();
                currentTrace.t_first_audio_playout = currentTrace.t_browser_first_playout;
                log('Agent playout (fallback: track subscribed + 500ms)', 'info');
                finalizeTrace();
              }
            }, 500);
          }
        });

        room.on(LivekitClient.RoomEvent.TrackUnsubscribed, (track) => {
          if (track.kind === 'audio') {
            track.detach().forEach(el => el.remove());
            setVisualizer('active');
          }
        });

        room.on(LivekitClient.RoomEvent.ActiveSpeakersChanged, (speakers) => {
          const agentSpeaking = speakers.some(s => s.identity !== (PARAM_IDENTITY || 'voice-user-1'));
          setVisualizer(agentSpeaking ? 'speaking' : 'active');
          if (agentSpeaking) onAgentAudioStart();
        });

        await room.connect(url, token);
        log('Publishing mic...', 'info');
        await room.localParticipant.setMicrophoneEnabled(true);
        log('Mic enabled', 'success');

        try {
          const micTrack = room.localParticipant.getTrackPublication(LivekitClient.Track.Source.Microphone);
          if (micTrack && micTrack.track) {
            const stream = new MediaStream([micTrack.track.mediaStreamTrack]);
            monitorMic(stream);
            log('Mic monitor started', 'info');
          }
        } catch(e) { log(`Mic monitor: ${e.message}`, 'info'); }

        startBrowserTrace();

        // D12 auto-disconnect timer
        if (AUTO_MODE && AUTO_DISCONNECT_S > 0) {
          setTimeout(async () => {
            log(`Auto-disconnect after ${AUTO_DISCONNECT_S}s`, 'event');
            stopRecording();
            await disconnect();
          }, AUTO_DISCONNECT_S * 1000);
        }

      } catch (e) {
        log(`Connect failed: ${e.message}`, 'error');
        setStatus('Error', 'error');
        window.__autobrowser_error = e.message;
        const btn = document.getElementById('connectBtn');
        if (btn) btn.disabled = false;
      }
    }

    async function disconnect() {
      stopRecording();
      if (room) {
        await room.disconnect();
        room = null;
        window.room = null;
      }
      if (agentAudioEl) { agentAudioEl.remove(); agentAudioEl = null; }
      if (micMonitorInterval) { clearInterval(micMonitorInterval); micMonitorInterval = null; }
      if (agentMonitorInterval) { clearInterval(agentMonitorInterval); agentMonitorInterval = null; }
      setStatus('Disconnected', 'disconnected');
      setVisualizer('');
      const cb = document.getElementById('connectBtn');
      if (cb) cb.disabled = false;
      const db = document.getElementById('disconnectBtn');
      if (db) db.disabled = true;
      log('Disconnected', 'event');
    }

    // D12: Auto-connect if auto mode
    if (AUTO_MODE && PARAM_TOKEN) {
      window.addEventListener('DOMContentLoaded', () => {
        setTimeout(() => connect(), 300);
      });
    }
  </script>
</body>
</html>
